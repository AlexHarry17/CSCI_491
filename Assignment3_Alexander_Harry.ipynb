{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_Alexander_Harry.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexHarry17/CSCI_491/blob/master/Assignment3_Alexander_Harry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzo8YMyn04bR",
        "colab_type": "text"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "The goal of this part is to explore some of the main [scikit-learn](http://scikit-learn.org/stable/index.html) tools on a single practical task: analysing a collection of text documents (newsgroups posts) on twenty different topics.\n",
        "\n",
        "In this section we will see how to:\n",
        "\n",
        "1. load the file contents and the categories\n",
        "2. extract feature vectors suitable for machine learning\n",
        "3. train a model to perform text classification\n",
        "4. evaluate the performance of the trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWLXr41H1kFz",
        "colab_type": "text"
      },
      "source": [
        "### Loading the 20 newsgroups dataset\n",
        "The 20 newsgroups dataset comprises around 20,000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). \n",
        "\n",
        "In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn.\n",
        "\n",
        "The **sklearn.datasets.fetch_20newsgroups** function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in a local folder and calls the **sklearn.datasets.load_files** on either the training or testing set folder, or both of them. Here, we are loading only 4 categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUpQ5bYq2GMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'] #select only 4 categories for fast running times\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats, remove=('headers', 'footers', 'quotes'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byNIxaqL2LyP",
        "colab_type": "text"
      },
      "source": [
        "We can now list the 4 categories as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZORgv8ozwtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(newsgroups_train.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-q-D5aP6JuR",
        "colab_type": "text"
      },
      "source": [
        "The real data lies in the **filenames** and **target** attributes. The target attribute is the integer index of the category:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4svhJW-6YEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(newsgroups_train.filenames[0]) #print the name of the first file\n",
        "print(newsgroups_train.target[0]) #print the category of the first example\n",
        "print(newsgroups_train.data[0]) #print the text of the first example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUE3z92Z73xt",
        "colab_type": "text"
      },
      "source": [
        "### Converting text to vectors\n",
        "In order to feed machine learing models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the **sklearn.feature_extraction.text** as demonstrated in the following example.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eb7r9SO8Y54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer #tokenizer\n",
        "vectorizer = CountVectorizer(stop_words='english') #remove english stop words\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "print (vectors.shape) #print the size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKjFkDVW9R6N",
        "colab_type": "text"
      },
      "source": [
        "### Training a machine learining model and evaluate its performance\n",
        " Let’s use a multinomial Naive Bayes classifier as discussed in the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VLmrBPW9e4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(vectors, newsgroups_train.target)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQXgKAlh96Ng",
        "colab_type": "text"
      },
      "source": [
        "Then let's print the F1 score on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv_ljFnG9tVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=cats, remove=('headers', 'footers', 'quotes'))  #test data\n",
        "vectors_test = vectorizer.transform(newsgroups_test.data)  #generate vectors from test data (using the same vectorizer)\n",
        "pred = clf.predict(vectors_test) #predict categories for the test data using the above trained classifier\n",
        "\n",
        "print(\"macro F1:\",metrics.f1_score(newsgroups_test.target, pred, average='macro'))\n",
        "print(\"micro F1:\",metrics.f1_score(newsgroups_test.target, pred, average='micro'))\n",
        "print(\"\\n\",metrics.classification_report(newsgroups_test.target, pred, target_names=newsgroups_test.target_names))\n",
        "cm = metrics.confusion_matrix(newsgroups_test.target, pred)\n",
        "print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We7-mPb_CSDq",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a look at what the most informative features are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlWPqCedCbh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def show_top10(classifier, vectorizer, categories):\n",
        "  feature_names = np.asarray(vectorizer.get_feature_names())\n",
        "  for i, category in enumerate(categories):\n",
        "    top10 = np.argsort(classifier.coef_[i])[-10:]\n",
        "    print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
        "\n",
        "show_top10(clf, vectorizer, newsgroups_train.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXgUMEq_uDpi",
        "colab_type": "text"
      },
      "source": [
        "Instead of train-test setup, you can also perform cross-validation (CV). Following code shows CV results using the train set (although you could do this with the complete dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPKe9JDSmCCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection  import cross_val_score\n",
        "cv_scores = cross_val_score(clf, vectors, newsgroups_train.target , cv=10, scoring=\"f1_macro\" )\n",
        "print(\"Avg. macro F1:\", np.mean(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVCZERLPdyYy",
        "colab_type": "text"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Ths goal of this part is to write your own code to train a model to classify the given test dataset using part 1 as inspiriation.\n",
        "\n",
        "First, upload the given dataset (\"diseases-train.csv\") using the following cell. It contains 900 scientific artciles (identified by their PubMed IDs) and their labels. This is a multi-class problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ru8k_nK05xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5jGiZKlW9Xa",
        "colab_type": "text"
      },
      "source": [
        "Then load the CSV file using pandas as below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80IHoQzj42MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"diseases-train.csv\")\n",
        "print(df_train.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wijrc25Y_tt",
        "colab_type": "text"
      },
      "source": [
        "Then you can iterate over the lines as follows. Each line has the format: pmid, category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyk6XDuCZITO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, row in df_train.iterrows():    \n",
        "    pmid = row[0]\n",
        "    print(pmid)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90r-99vPZTLh",
        "colab_type": "text"
      },
      "source": [
        "Then you can get the other information (i.e. title, abstract etc) associated with each of these articles using the [biopython](https://biopython.org/) library. First insatll the library as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EV8JFxDgSoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install biopython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Udn-Lu9alvC",
        "colab_type": "text"
      },
      "source": [
        "You can fetch the information for an article with the **eftech** function as below. Find more information [here](http://biopython.org/DIST/docs/tutorial/Tutorial.html#sec:efetch). \n",
        "\n",
        "(Note: You can search for pubmed articles using keywords using [esearch](http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc123). This may be useful for your projects.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR1ekZbhat9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bio import Entrez, Medline\n",
        "Entrez.email = \"alexander.harry1@ecat1.montana.edu\"  # Always tell NCBI who you are\n",
        "handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"medline\", retmode=\"text\")\n",
        "print(handle.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcY9zW1tbgf_",
        "colab_type": "text"
      },
      "source": [
        "The decription of the medline format is [here](https://www.nlm.nih.gov/bsd/disted/pubmedtutorial/030_080.html). You can parse this *handle* using the **Medline.parse** function. More informaton is [here](https://biopython.org/DIST/docs/api/Bio.Medline-module.html).\n",
        "\n",
        "Once you grab enough information about each article from pubmed, your task is to train a model uisng the given data and make predictions for the articles in test data (\"diseases-test-without-labels.csv\"). In order to find the best parameter values for your models, you will split the given data into train-1 and train-2. Then you will use train-1 to train your models and train-2 to test your models (typically, train-2 is called the dev set or validation set). This way, you are able to find the best model for making final predictions. \n",
        "\n",
        "You can add one more code/text blocks for answering the follwoing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9No8gvCdXv1",
        "colab_type": "text"
      },
      "source": [
        "1. Details about datasets: What is the label distribution in the full dataset? What are the sizes of train-1 and train-2 datasets you used (and their individual label distriutions)? Also, show the distribution(s) visually.\n",
        "\n",
        "(Note: you can create bar plots or pie charts using the following:\n",
        "[matplotlib.pyplot.bar](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.bar.html), \n",
        "[matplotlib.pyplot.pie](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pie.html)) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJaiYNNEgLCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_train['category'].unique() # List of unique labels\n",
        "\n",
        "def label_distrobution(distro_description, data): # Calculates and plots the label distrobutions.\n",
        "  label_distro = {}\n",
        "  print(\"\\n------------------------\", distro_description, \"Label Distrobution------------------------\")\n",
        "  for label in labels: # Loops through the labels to create a dictionary of counts for each label\n",
        "    label_distro[label] = list(data['category']).count(label) # creates a key/count value for each label\n",
        "    print(\"Label: \", label, \" Count: \", label_distro[label], \" Distrobution: \", str(label_distro[label] / len(data)) + \"%\") # Prints the distrobution.\n",
        "\n",
        "  print(\"\\n------------------------\", distro_description, \"Label Distrobution Plot------------------------\")\n",
        "  # Plotting dictionary sourced from https://stackoverflow.com/questions/21195179/plot-a-histogram-from-a-dictionary/21195331 from User: Alvaro Fuentes\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.bar(label_distro.keys(), label_distro.values()) # Plots the bar chart of distrobutions. -- Source above--\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFqWggLZSEHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Split the training sets\n",
        "split = int(len(df_train) * 0.8) # Split the training data 80/20\n",
        "\n",
        "train_1 = df_train[:split][:] \n",
        "train_2 = df_train[split:][:]\n",
        "\n",
        "print(\"\\n------------------------Training Split------------------------\")\n",
        "print(\"Train-1 size: \", len(train_1), \" Train-2 size: \", len(train_2))\n",
        "\n",
        "# Print the label distrobutions for each data set.\n",
        "label_distrobution(\"Full Data Set\", df_train)\n",
        "label_distrobution(\"Train-1\", train_1)\n",
        "label_distrobution(\"Train-2\", train_2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDfPap3UDwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bio import Entrez, Medline\n",
        "\n",
        "def parseData(data, train): # Method to parse the data\n",
        "  data_set = [] # For the new data set\n",
        "  label = [] # List of labels\n",
        "  for pmid_val in data.iterrows(): # Iterate through the rows of the data passed in\n",
        "   if train == \"True\":\n",
        "     handle = Entrez.efetch(db=\"pubmed\", id=pmid_val[1][0], rettype=\"medline\", retmode=\"text\") # Sourced from the code provided above. Grabs the handle of the specific pmid.\n",
        "   else:\n",
        "     handle = Entrez.efetch(db=\"pubmed\", id=int(pmid_val[1]['pmid']), rettype=\"medline\", retmode=\"text\") # Sourced from the code provided above. Grabs the handle of the specific pmid.\n",
        "   parsed = Medline.parse(handle) # Parses the handle\n",
        "   for parse in parsed:\n",
        "    if 'TI' in parse.keys(): # Checks that the text exists\n",
        "      if train == \"True\": # Creates a data set for labels if data is the training set\n",
        "        label.append(pmid_val[1][1])\n",
        "      text = parse['TI'] \n",
        "\n",
        "      data_set.append(text) # Appends to the dataset\n",
        "  if train == \"True\":\n",
        "    return data_set, label\n",
        "  else:\n",
        "    return data_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJWvjp1svVnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_1_data, train_1_labels = parseData(train_1, True)\n",
        "# print(\"Text: \", train_1_data)\n",
        "# print(\"Labels: \", train_1_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2padtJ64yAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_2_data, train_2_labels = parseData(train_2, True)\n",
        "# print(\"Text: \", train_2_data)\n",
        "# print(\"Labels: \", train_2_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0BB-Y5jskIR",
        "colab_type": "text"
      },
      "source": [
        "2. Data preprocessing:\n",
        "How did you preprocess you data? You can do stemming/lemmatization using [NLTK](https://www.nltk.org/) library:\n",
        "[stemming](https://www.nltk.org/api/nltk.stem.html), \n",
        "[lemmatization](https://www.nltk.org/_modules/nltk/stem/wordnet.html). Make sure to apply the same pre-processing to both train-1 and train-2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqP3wiZ0jP-V",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: I preprocessed the data by parsing with Medline.parse.  I created a list of text from parsing the 'TI' key and created a list of labels associated with the list.  The data then has chars like '.' replaced, and is then lemmatized and stemmed using the SnowballStemmer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1g0YnWgkl7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Help with stemming and lemmatization from source https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\n",
        "\n",
        "# temp_train_1 = train_1_data.copy()\n",
        "# temp_train_2 = train_2_data.copy()\n",
        "\n",
        "# temp_labels_1 = train_1_labels.copy()\n",
        "# temp_labels_2 = train_2_labels.copy()\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import nltk \n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Help with LancasterStemmer and SnowballStemmer code sourced from https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\n",
        "# stemmer = LancasterStemmer()\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "# Help with stemming and lemmatization from source https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\n",
        "def stemming(data): # Method for stemming the data\n",
        "  for text in range(len(data)): # Loops through the text.\n",
        "    line = data[text]\n",
        "    line = line.split() # Splits the line at ' ' into a list\n",
        "    new_line = [] # Temp list to join later\n",
        "    for word in line: # Loops through the list of words in the sentence\n",
        "      new_line.append(stemmer.stem(word)) # Stems and appends to the list.\n",
        "    data[text] = ' '.join(new_line) # Join method sourced from https://stackoverflow.com/questions/29642188/removing-the-square-brackets-commas-and-single-quote, user: halex \n",
        "  return data\n",
        "  \n",
        "def replace(data): # Replaces line breaks with ''.\n",
        "  chars = ['.', '[',']', \"'\", '?', '!', '(', ')', ':', ';' ] # The chars to remove from the sentence\n",
        "  for text in range(len(data)): # Loops through the text \n",
        "    for char in chars: # Loops through the items to replace\n",
        "      data[text] = data[text].replace(char, '') # Replace method sourced from https://www.journaldev.com/23674/python-remove-character-from-string\n",
        "  return data\n",
        "\n",
        "def lemmatization(data): # Method to lemmatize the data.  Sourced from https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\n",
        "  for text in range(len(data)): # Loops through the text.\n",
        "    line = data[text]\n",
        "    line = line.split() # Splits the line at ' ' into a list\n",
        "    new_line = [] # Temp list to join later\n",
        "    for word in line: # Loops through the list of words in the sentence\n",
        "      new_line.append(lemmatizer.lemmatize(word)) # Lematize and appends to the list.\n",
        "    data[text] = ' '.join(new_line) # Join method sourced from https://stackoverflow.com/questions/29642188/removing-the-square-brackets-commas-and-single-quote, user: halex \n",
        "  return data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehv0I2--5qo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace, lemmatize, and stem the text\n",
        "def prep_data(data, has_labels): # Method to prep the data\n",
        "  if has_labels == \"True\":\n",
        "    data, labels = parseData(data, has_labels) # Parses the data\n",
        "  else:\n",
        "    data = parseData(data, has_labels) # Parses the data\n",
        "\n",
        "  data = replace(data) # Replace chars we don't want in the string\n",
        "  data = lemmatization(data) # Lemmatize the data\n",
        "  data = stemming(data) # Stem the data.\n",
        "\n",
        "  if has_labels == \"True\":\n",
        "    return data, labels\n",
        "  else:\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9HlNs1gV5gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_1_data, train_1_labels = prep_data(train_1, \"True\")\n",
        "train_2_data, train_2_labels = prep_data(train_2, \"True\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SU27Xt2vQAH",
        "colab_type": "text"
      },
      "source": [
        "3. Features used:\n",
        "What feature model was used? Chekout all the options for [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Also, instead of the Bag-of-words (CountVectorizer) model you can optionally use [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). You can read more about both these approches [here](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Regarless of the Vectorizer used, list all paramer values you used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DF-GvX-i6Zn",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: I tried using both the CountVectorizer and TfidVectorizer feature model.  I found that both were similar, but for this particular dataset, the TfidVectorizer performed slightly better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBGKloLK5xDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following code for the TfidVectorizer is sourced and used from the examples at the source: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "vectorizer = TfidfVectorizer('english')\n",
        "# vectorizer = CountVectorizer('english')\n",
        "\n",
        "data_vectors = vectorizer.fit_transform(train_1_data) # Turn the data into a vector\n",
        "print(data_vectors.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgFJcO6gwVqJ",
        "colab_type": "text"
      },
      "source": [
        "4. NB Model performance:\n",
        "Report the performance values using Naive Bayes here. What is represented by *alpha* and *fit_prior* parameters? What value pair for *alpha* (try 0.0 or 1.0) and *fit_prior* (try True or False) parameters gives you the best overall performance in terms of macro-avreaged F1 (train using train-1 and test using train-2). For the best performing model, show confusion matrix. Show individual F1 values for each category in a bar chart. What are the categories that are easiest/ hardest to predict?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKAnVBy2naVm",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: According to source : https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html,  The aplpha parameter smooths the data, while the fit_prior parameter is a boolean that decides \"Whether to learn class prior probabilities or not\".  Changing the alpha and fit_prior values slightly decreases performance of my algorithm, though it is only a slight amount. The easiest category to predict is eye diseases.  The hardest category to predict is skin diseases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dad1pVIH6efx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Naive bayes sourced from the code block above in part 1\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "classifier = MultinomialNB() # Create the naive bayes classifier\n",
        "classifier.fit(data_vectors, train_1_labels) # Fit the data to the classifier."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn0n2xcM8g-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code sourced from code block above in part 1.\n",
        "data_vectors_test = vectorizer.transform(train_2_data)  # Turn the data into a vector\n",
        "predicted = classifier.predict(data_vectors_test) # Get the predictions on the test set.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQFqd_E3dLMP",
        "colab_type": "text"
      },
      "source": [
        "# The Following five commented blocks are from testing different stemmers/vectorizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb0lakL6oRwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The following code is sourced from the above code block in part 1.\n",
        "# from sklearn import metrics\n",
        "# # Snowball Stemmer\n",
        "# # TfidVectorizer\n",
        "# #Alpha - 1.0\n",
        "# # Fit_prior = False\n",
        "\n",
        "# print(\"macro F1:\",metrics.f1_score(train_2_labels, predicted, average='macro'))\n",
        "# print(\"micro F1:\",metrics.f1_score(train_2_labels, predicted, average='micro'))\n",
        "# print(\"\\n\",metrics.classification_report(train_2_labels, predicted))\n",
        "# cm = metrics.confusion_matrix(train_2_labels, predicted)\n",
        "# print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MewGq6XUn6ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The following code is sourced from the above code block in part 1.\n",
        "# from sklearn import metrics\n",
        "# # Snowball Stemmer\n",
        "# # TfidVectorizer\n",
        "# # Alpha - 0.1\n",
        "# # Fit_prior = True\n",
        "\n",
        "\n",
        "# print(\"macro F1:\",metrics.f1_score(train_2_labels, predicted, average='macro'))\n",
        "# print(\"micro F1:\",metrics.f1_score(train_2_labels, predicted, average='micro'))\n",
        "# print(\"\\n\",metrics.classification_report(train_2_labels, predicted))\n",
        "# cm = metrics.confusion_matrix(train_2_labels, predicted)\n",
        "# print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scm1vhD29Gd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn import metrics\n",
        "# # Lancaster Stemmer\n",
        "# # CountVectorizer\n",
        "# The following code is sourced from the above code block in part 1.\n",
        "\n",
        "# print(\"macro F1:\",metrics.f1_score(temp_labels_2, predicted, average='macro'))\n",
        "# print(\"micro F1:\",metrics.f1_score(temp_labels_2, predicted, average='micro'))\n",
        "# print(\"\\n\",metrics.classification_report(temp_labels_2, predicted))\n",
        "# cm = metrics.confusion_matrix(temp_labels_2, predicted)\n",
        "# print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBHf41u9CR3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn import metrics\n",
        "# # Lancaster Stemmer\n",
        "# # TfidVectorizer\n",
        "# The following code is sourced from the above code block in part 1.\n",
        "\n",
        "# print(\"macro F1:\",metrics.f1_score(temp_labels_2, predicted, average='macro'))\n",
        "# print(\"micro F1:\",metrics.f1_score(temp_labels_2, predicted, average='micro'))\n",
        "# print(\"\\n\",metrics.classification_report(temp_labels_2, predicted))\n",
        "# cm = metrics.confusion_matrix(temp_labels_2, predicted)\n",
        "# print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAhj1ccw_pMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # This code is sourced from above in part 1.\n",
        "# from sklearn import metrics # Import the metrics library\n",
        "# # Snowball Stemmer\n",
        "# # CountVectorizer\n",
        "\n",
        "# print(\"macro F1:\",metrics.f1_score(train_2_labels, predicted, average='macro'))\n",
        "# print(\"micro F1:\",metrics.f1_score(train_2_labels, predicted, average='micro'))\n",
        "# print(\"\\n\",metrics.classification_report(train_2_labels, predicted))\n",
        "# cm = metrics.confusion_matrix(train_2_labels, predicted)\n",
        "# print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrEl9zIKCm9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "# Snowball Stemmer\n",
        "# TfidVectorizer\n",
        "# Alpha - 1.0\n",
        "# Fit_prior = True\n",
        "\n",
        "# The following code is sourced from the above code block in part 1.\n",
        "# Print metrics of the predictions\n",
        "print(\"macro F1:\",metrics.f1_score(train_2_labels, predicted, average='macro'))\n",
        "print(\"micro F1:\",metrics.f1_score(train_2_labels, predicted, average='micro'))\n",
        "print(\"\\n\",metrics.classification_report(train_2_labels, predicted))\n",
        "cm = metrics.confusion_matrix(train_2_labels, predicted)\n",
        "print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmGqrPXLpChH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_fScore(f_label, prediction): # Returns the f-score and labels in two arrays\n",
        "  f_score = metrics.precision_recall_fscore_support(f_label, prediction) # Grabs the metrics from the fscore report with help of https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support\n",
        "  f_score = f_score[2] # Array of f_scores pulled from the metrics library\n",
        "  # code for unique values as a np array sourced from: https://www.geeksforgeeks.org/python-get-unique-values-list/\n",
        "  labels_fscore = np.unique(np.array(f_label)) # Gets the unique values of the labels\n",
        "  return f_score, labels_fscore\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f_score_NB, f_score_labels_NB = get_fScore(train_2_labels, predicted) \n",
        "\n",
        "# Label axis of graph\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.bar(f_score_labels_NB, f_score_NB) # Plots the bar chart of the f-score distrobutions\n",
        "plt.show() # Display the chart"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tGC-5yYqVTX2"
      },
      "source": [
        "5. Other Model performance:\n",
        "Comapre NB performance to at least one other model mentioned in the class (e.g. KNN). Which model did you pick? List all parameter values you selected. Show the individual F1 values for each category for the two models side-by-side in a bar chart.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liVz6alhdtMf",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: The model I picked was a KNeighborsClassifier.  All of the parameters are the default except for changing the K value to be 10 neighbors.  It was set to 10 as that was the best result I found.  I tried different algorithms without much change in results.  The algorithms params tried were auto, ball_tree, kd_tree , and brute.  I played with manhattan_distance, euclidean distance, and minkowski.  I found euclidean and minkowski to be pretty close, and manhattan to reduce results.  I then tried the two weight options, 'uniform' and 'distance' and found 'distance' to perform slightly better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnUpEEsTD8bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source for how to do the following KNN code : https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors=10, weights='distance') # Create the naive bayes classifier\n",
        "classifier.fit(data_vectors, train_1_labels) # Fit the data to the classifier.\n",
        "\n",
        "predicted = classifier.predict(data_vectors_test) # predict categories for the test data using the above trained classifier\n",
        "\n",
        "from sklearn import metrics\n",
        "# Snowball Stemmer\n",
        "# TfidVectorizer\n",
        "\n",
        "# The following code is sourced from the above code block in part 1.\n",
        "# Print prediction metrics\n",
        "print(\"macro F1:\",metrics.f1_score(train_2_labels, predicted, average='macro'))\n",
        "print(\"micro F1:\",metrics.f1_score(train_2_labels, predicted, average='micro'))\n",
        "print(\"\\n\",metrics.classification_report(train_2_labels, predicted))\n",
        "cm = metrics.confusion_matrix(train_2_labels, predicted)\n",
        "print(\"Confusion Matrix:\\n\",cm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSiJw_nKUsZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_score_KNN, f_score_labels_KNN = get_fScore(train_2_labels, predicted)  # Grab the KNN F-Scores\n",
        "\n",
        "# Following code for how to do the side by side bar chart sourced from : https://pythonspot.com/matplotlib-bar-chart/\n",
        "x_location = np.arange(4) # Grabs the x location to plot\n",
        "width = 0.3 # Width of the bars\n",
        "rect_nb = plt.bar(x_location, f_score_KNN, color=\"blue\", label = 'Naive Bayes', width=width) # Plots the naive bayes bars\n",
        "rect_knn = plt.bar(x_location + width, f_score_NB, color=\"red\", label = 'KNN', width=width) # Plots the KNN bars\n",
        "\n",
        "# Label the chart.\n",
        "plt.ylabel('F1-Score')\n",
        "plt.xlabel('Class')\n",
        "plt.xticks(x_location + (width/2), f_score_labels_NB) # Adds the class labels to the bottem of the chart\n",
        "plt.legend() # Adds the legend to the chart.\n",
        "plt.tight_layout() # Looks better with this.\n",
        "plt.show() # Show the plot.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FWQcjFjWNww",
        "colab_type": "text"
      },
      "source": [
        "6. Predict the categories for articles in the given test data set (\"diseases-test-without-labels.csv\"). Save the predictions into \"diseases-test-preds.csv\" (it should have the exact same format as the \"diseases-train.csv\" file) and upload along with the completed ipynb file. We will use your generated \"diseases-test-preds.csv\" file for evaluting the final perforamnce of your code (using macro-averaged F1). \n",
        "\n",
        "The three best performing submissions on test data will get **bonus points** (5% of the assignment grade for the 1st place, 3% for 2nd, and 2% for 3rd). The winner will be annouced in the class afther final evalution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCL7KWFvISro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code sourced from above block in part 1.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puXgr8z9SB4f",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCvFFQQ_IwkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(\"diseases-test-without-labels.csv\") # Reads the csv file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFksG7iEMuX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = prep_data(df_test, \"False\") # Prepares the test Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYfEbnCRYR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Following sourced from code above in part 1\n",
        "vector = vectorizer.transform(test_data) # Create a vector of the test\n",
        "predicted_values = classifier.predict(vector) # Predict classes\n",
        "pmid = list(df_test['pmid']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NOwFCcbzFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "row_list = [['pmid', 'category']] # Initializes the list with labels\n",
        "for row in range(len(predicted_values)): # Appends a list to add to a csv\n",
        "  row_list.append([pmid[row], predicted_values[row]])\n",
        "\n",
        "# The following code to write to csv is written with the help from source : https://www.programiz.com/python-programming/working-csv-files\n",
        "import csv\n",
        "with open('diseases-test-preds.csv', 'w') as file: # Creates and opens the writable csv file\n",
        "  csv.writer(file).writerows(row_list) # Writes to the csv file.  Each row is an instance in the row_list created prior.\n",
        "file.close() # Closes the file.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoIx56Y_il3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}